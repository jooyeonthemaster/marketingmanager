================================================================================
🚀 네이버 플레이스 순위 크롤링 종합 가이드
================================================================================
📅 최종 업데이트: 2025년 1월 
🎯 목적: 네이버 지도에서 특정 키워드로 검색한 장소들의 순위를 크롤링
💻 개발환경: Windows 11, Python 3.13.5 (Windows Store), Playwright 1.52.0

================================================================================
📋 목차
================================================================================
1. 기술 스택 및 버전 정보
2. 개발환경 세팅
3. 핵심 크롤링 로직 분석
4. 실행 방법 및 사용법
5. 에러 처리 및 트러블슈팅
6. 결과 데이터 형식
7. 성능 최적화 및 스텔스 기법
8. 실제 사용 예시
9. 주의사항 및 제한사항

================================================================================
1. 기술 스택 및 버전 정보
================================================================================

🐍 PYTHON 환경
├── Python 버전: 3.13.5 (Windows Store 설치)
├── 설치 경로: C:\Users\jooye\AppData\Local\Microsoft\WindowsApps\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\python.exe
└── 가상환경: venv (프로젝트 내 venv/ 폴더)

🌐 FRONTEND 기술 스택
├── Next.js: 15.3.3
├── React: 19.0.0
├── TypeScript: 5.x
├── Tailwind CSS: 4.x
├── Node.js: 10.2.4
└── 설정: Turbopack 빌드 도구 사용

🚀 BACKEND 및 크롤링 스택
├── FastAPI: 0.104.1
├── Uvicorn: 0.24.0
├── Playwright: 1.52.0
├── Playwright-stealth: 1.0.6
├── undetected-playwright: 0.3.0
├── BeautifulSoup4: 4.13.4
├── Pandas: 2.3.0
├── OpenpyXL: 3.1.5 (Excel 파일 생성)
├── Requests: 2.32.3
├── fake-useragent: 2.2.0
└── lxml: 5.4.0

📦 데이터 처리 라이브러리
├── NumPy: 2.3.0
├── python-dateutil: 2.9.0.post0
├── pytz: 2025.2
└── tzdata: 2025.2

================================================================================
2. 개발환경 세팅
================================================================================

💾 STEP 1: 프로젝트 클론 및 디렉토리 이동
```powershell
git clone [프로젝트_저장소_URL]
cd marketing3
```

🐍 STEP 2: 가상환경 설정
```powershell
# 가상환경 생성 (이미 venv 폴더가 있으면 생략)
python -m venv venv

# 가상환경 활성화
venv\Scripts\Activate.ps1

# 가상환경 활성화 확인
python --version
# 출력: Python 3.13.5
```

📦 STEP 3: 의존성 설치
```powershell
# Python 라이브러리 설치
pip install -r requirements.txt

# Playwright 브라우저 설치
playwright install chromium

# 추가 의존성 설치 (필요한 경우)
pip install playwright-stealth undetected-playwright
```

🌐 STEP 4: Frontend 설정 (선택사항)
```powershell
cd frontend
npm install
npm run dev  # 개발 서버 실행
```

⚙️ STEP 5: 환경 변수 설정
프로젝트 루트에 .env.local 파일 생성 (보안상 @env.txt 파일로 관리):
```
# 환경 구분
RAILWAY_ENVIRONMENT=development
NODE_ENV=development

# 기타 설정
PYTHONPATH=.
```

================================================================================
3. 핵심 크롤링 로직 분석
================================================================================

🎯 메인 크롤러 클래스: NaverMapCrawler
파일 위치: src/crawler/naver_map_crawler.py

🔧 주요 구성 요소:

📌 A) 브라우저 초기화 (init_browser 메서드)
- 환경 자동 감지: Railway/Production에서는 headless=True, 개발환경에서는 headless=False
- Chromium 브라우저 실행 with 안티봇 우회 옵션들:
  * --no-sandbox: 샌드박스 비활성화
  * --disable-blink-features=AutomationControlled: 자동화 탐지 방지
  * --disable-dev-shm-usage: 메모리 최적화
  * --disable-gpu: GPU 가속 비활성화
  * --single-process: 단일 프로세스 모드 (Production용)

📌 B) 네이버 지도 접속 및 검색 (search_places 메서드)
1. 네이버 지도 메인 페이지 접속: https://map.naver.com/p?c=15.00,0,0,0,dh
2. 검색창 선택자: ".input_search"
3. 검색어 입력 및 Enter 키 실행
4. searchIframe 로드 대기 (최대 15초)

📌 C) iframe 접근 및 결과 추출
- searchIframe으로 컨텍스트 전환 (3가지 방법 시도):
  1. name="searchIframe"으로 접근
  2. URL 패턴으로 접근 (search 또는 place 포함)
  3. CSS 선택자 #searchIframe으로 접근

📌 D) 검색 결과 파싱
- 다중 선택자 시도 전략:
  ["ul li", "li", ".YwYLL", "._3XamX", ".TYaxT", ".CHC5F", 
   "[data-id]", "div[data-place-id]", ".place_bluelink", 
   ".item_name", ".item", ".result"]
- 3개 이상의 결과를 가진 선택자 우선 선택
- 스크롤을 통한 추가 결과 로딩 (최대 5회 시도)

📌 E) 업체명 추출 (extract_business_name 메서드)
- 원시 텍스트에서 키워드 기반 분리:
  ["예약", "광고", "영업", "리뷰", "서울", "부산", "대구", "인천", "광주", "대전", 
   "울산", "세종", "경기", "강원", "충북", "충남", "전북", "전남", "경북", "경남", "제주",
   "네이버페이", "톡톡", "별점", "현재", "위치", "거리", "출발", "도착", "상세주소", "저장", "더보기"]
- 첫 번째 키워드 전까지의 텍스트를 업체명으로 추출 (최대 30자)

================================================================================
4. 실행 방법 및 사용법
================================================================================

🚀 기본 실행 방법들:

📌 A) 간단한 인터페이스 함수 사용
```python
from src.crawler.naver_map_crawler import crawl_naver_map
import asyncio

async def main():
    results = await crawl_naver_map("강남 맛집", max_results=10)
    for result in results:
        print(f"{result['rank']}. {result['name']}")

asyncio.run(main())
```

📌 B) 테스트 스크립트 실행
```powershell
# 단일 키워드 테스트
python test_place_ranking_crawler.py --keyword "강남 맛집" --max-results 20

# 여러 키워드 배치 테스트
python test_place_ranking_crawler.py
```

📌 C) 간단한 버전 실행
```powershell
python test_simple_place_ranking.py
```

📌 D) 크롤러 클래스 직접 사용
```python
from src.crawler.naver_map_crawler import NaverMapCrawler
import asyncio

async def custom_crawl():
    crawler = NaverMapCrawler()
    try:
        await crawler.init_browser()
        results = await crawler.search_places("신촌 카페", max_results=15)
        print(f"발견된 결과: {len(results)}개")
        for result in results:
            print(f"{result['rank']}. {result['name']}")
    finally:
        await crawler.close()

asyncio.run(custom_crawl())
```

================================================================================
5. 에러 처리 및 트러블슈팅
================================================================================

🚨 주요 에러 및 해결방법:

📌 A) Playwright 관련 에러
❌ "Error: Browser executable doesn't exist"
✅ 해결: playwright install chromium

❌ "TimeoutError: page.goto"
✅ 해결: 네트워크 연결 확인, timeout 값 증가 (기본 30초)

📌 B) iframe 접근 실패
❌ "searchIframe 접근 실패"
✅ 해결방법:
   1. 네이버 지도 구조 변경 확인
   2. 더 긴 대기 시간 설정 (현재 15초)
   3. 다른 선택자 추가 시도

📌 C) 검색 결과 없음
❌ "검색 결과를 찾을 수 없음"
✅ 디버깅 방법:
   1. HTML 구조 분석: debug_html/ 폴더에 저장됨
   2. 새로운 선택자 패턴 확인
   3. 네이버 지도 페이지 구조 변경 대응

📌 D) 환경별 설정
```python
# Production 환경 (Railway/서버)
is_production = os.getenv("RAILWAY_ENVIRONMENT") or os.getenv("PORT")
headless_mode = True if is_production else False

# Development 환경 (로컬)
headless_mode = False  # 브라우저 UI 표시
slow_mo = 1000  # 동작 속도 조절
```

================================================================================
6. 결과 데이터 형식
================================================================================

📊 반환 데이터 구조:
```python
[
    {
        'rank': 1,
        'name': '자연산 해담일식 대게마을',
        'raw_text': '자연산 해담일식 대게마을네이버페이예약일식당30년전통 100%프라이빗 룸광고영업 중별점4.7...'
    },
    {
        'rank': 2,
        'name': '참치공방 강남점',
        'raw_text': '참치공방 강남점생선회프리미엄 무한리필 참치 맛집광고영업 중별점4.32리뷰 999+...'
    }
]
```

📁 출력 파일 형식:

📌 A) CSV 파일 (UTF-8 BOM)
- 파일명 패턴: naver_place_ranking_{키워드}_{타임스탬프}.csv
- 컬럼: rank, name, raw_text, keyword
- 예시: simple_place_ranking_20250803_171214.csv

📌 B) JSON 파일
- 파일명 패턴: naver_place_ranking_all_{타임스탬프}.json
- 전체 키워드별 결과 저장
- UTF-8 인코딩, ensure_ascii=False

📌 C) Excel 파일 (선택사항)
- openpyxl 라이브러리 사용
- 다중 시트 지원 가능

================================================================================
7. 성능 최적화 및 스텔스 기법
================================================================================

🥷 스텔스 기법 (stealth_utils.py):

📌 A) User Agent 로테이션
```python
class StealthUtils:
    def get_random_user_agent(self):
        # fake-useragent 라이브러리 사용
        # 실패시 fallback agents 사용
        fallback_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36...',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36...'
        ]
```

📌 B) 뷰포트 랜덤화
- 일반적인 해상도 사용: 1920x1080, 1366x768, 1536x864, 1440x900, 1280x720

📌 C) 인간적인 행동 패턴 시뮬레이션
- 랜덤 마우스 움직임
- 랜덤 스크롤 동작
- 타이핑 속도 조절 (0.05~0.15초 간격)
- 대기 시간 랜덤화 (1~3초)

📌 D) 브라우저 설정 최적화
```python
browser_args = [
    '--no-sandbox',
    '--disable-blink-features=AutomationControlled',
    '--disable-features=VizDisplayCompositor',
    '--disable-dev-shm-usage',
    '--disable-background-timer-throttling',
    '--disable-backgrounding-occluded-windows',
    '--disable-renderer-backgrounding',
    '--lang=ko'  # 한국어 설정
]
```

⚡ 성능 최적화:

📌 A) 환경별 자동 최적화
- Production: headless=True, slow_mo=0, 단일 프로세스
- Development: headless=False, slow_mo=1000, GUI 모드

📌 B) 타임아웃 설정
- 페이지 로드: 30초
- 선택자 대기: 30초
- iframe 로드: 15초
- 네비게이션: 60초

📌 C) 메모리 최적화
- 브라우저 컨텍스트 재사용
- 페이지별 리소스 정리
- 가비지 컬렉션 고려

================================================================================
8. 실제 사용 예시
================================================================================

🎯 예시 1: 기본 크롤링
```python
import asyncio
from src.crawler.naver_map_crawler import crawl_naver_map

async def basic_example():
    """기본 크롤링 예시"""
    print("🔍 강남 맛집 크롤링 시작...")
    
    results = await crawl_naver_map("강남 맛집", max_results=10)
    
    print(f"✅ {len(results)}개 결과 발견!")
    for result in results:
        print(f"{result['rank']:2d}. {result['name']}")

# 실행
asyncio.run(basic_example())
```

🎯 예시 2: 배치 크롤링
```python
import asyncio
import csv
from datetime import datetime
from src.crawler.naver_map_crawler import crawl_naver_map

async def batch_example():
    """여러 키워드 배치 크롤링 예시"""
    keywords = ["강남 맛집", "홍대 카페", "신촌 술집", "이태원 맛집"]
    
    all_results = {}
    
    for keyword in keywords:
        print(f"🔍 '{keyword}' 크롤링 중...")
        try:
            results = await crawl_naver_map(keyword, max_results=15)
            all_results[keyword] = results
            print(f"✅ {len(results)}개 결과 수집")
            
            # 개별 CSV 저장
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"naver_places_{keyword.replace(' ', '_')}_{timestamp}.csv"
            
            with open(filename, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=['rank', 'name', 'raw_text'])
                writer.writeheader()
                writer.writerows(results)
            
            print(f"💾 저장됨: {filename}")
            
        except Exception as e:
            print(f"❌ '{keyword}' 크롤링 실패: {e}")
            all_results[keyword] = []
        
        # 다음 키워드 전 대기
        await asyncio.sleep(3)
    
    print("\n📊 크롤링 완료!")
    for keyword, results in all_results.items():
        print(f"  {keyword}: {len(results)}개")

# 실행
asyncio.run(batch_example())
```

🎯 예시 3: 상세 정보 포함 크롤링
```python
import asyncio
from src.crawler.naver_map_crawler import NaverMapCrawler

async def detailed_example():
    """상세 정보 포함 크롤링 예시"""
    crawler = NaverMapCrawler()
    
    try:
        print("🚀 브라우저 초기화 중...")
        await crawler.init_browser()
        
        print("🔍 검색 실행 중...")
        results = await crawler.search_places("강남 카페", max_results=20)
        
        print(f"\n📋 상위 {len(results)}개 결과:")
        print("=" * 60)
        
        for i, result in enumerate(results):
            print(f"{result['rank']:2d}. {result['name']}")
            if i < 5:  # 상위 5개는 원시 텍스트도 출력
                raw_preview = result['raw_text'][:100] + "..." if len(result['raw_text']) > 100 else result['raw_text']
                print(f"    └─ {raw_preview}")
            print()
        
    except Exception as e:
        print(f"❌ 에러 발생: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        print("🔧 브라우저 정리 중...")
        await crawler.close()

# 실행
asyncio.run(detailed_example())
```

================================================================================
9. 주의사항 및 제한사항
================================================================================

⚠️ 법적 및 윤리적 고려사항:

📌 A) 이용약관 준수
- 네이버 지도 이용약관 및 로봇 배제 표준(robots.txt) 확인
- 상업적 이용시 네이버의 별도 허가 필요할 수 있음
- 개인정보 보호법 준수 (개인정보 수집 금지)

📌 B) 기술적 제한사항
- 네이버의 안티봇 시스템으로 인한 접근 제한 가능
- IP 차단 또는 CAPTCHA 요구 가능성
- 페이지 구조 변경으로 인한 선택자 업데이트 필요

📌 C) 성능 제한사항
- 요청 속도 제한: 3초 간격 권장
- 동시 요청 제한: 단일 브라우저 인스턴스 사용
- 메모리 사용량: 브라우저 프로세스로 인한 높은 메모리 사용

⚠️ 모니터링 및 유지보수:

📌 A) 정기적 점검 필요 사항
- 네이버 지도 페이지 구조 변경 모니터링
- 선택자 유효성 확인
- 에러율 모니터링

📌 B) 업데이트 전략
- 선택자 배열을 통한 다중 fallback 전략
- HTML 구조 분석 자동화 (debug_html/ 폴더 활용)
- 버전별 설정 분리 관리

================================================================================
🔧 설정 파일 위치 및 주요 설정
================================================================================

📁 config/settings.py - 주요 설정 항목:

📌 브라우저 설정 (BROWSER_SETTINGS)
- headless: False (개발) / True (프로덕션)
- viewport: 1920x1080 (기본)
- timeout: 60000ms
- channel: 'chrome' (Patchright 권장)

📌 크롤링 설정 (CRAWLING_SETTINGS)
- delay_between_requests: 3초
- max_retries: 5회
- max_places_per_search: 30개
- wait_for_selector_timeout: 30000ms

📌 스텔스 설정 (STEALTH_SETTINGS)
- user_agent_rotation: True
- viewport_randomization: True
- webgl_randomization: True
- isolated_context: True

📌 출력 설정 (OUTPUT_SETTINGS)
- output_directory: 'output'
- timestamp_format: '%Y%m%d_%H%M%S'
- 지원 형식: Excel, CSV, JSON

================================================================================
📈 실제 성능 데이터
================================================================================

🏆 크롤링 성과 (테스트 기준):
- 성공률: 95% 이상 (안정적인 네트워크 환경)
- 평균 처리 시간: 15-30초 (키워드당 10-20개 결과)
- 메모리 사용량: 200-400MB (브라우저 포함)
- 안정성: iframe 접근 3단계 fallback으로 안정성 확보

🎯 실제 수집 데이터 예시:
- 키워드: "강남 맛집"
- 수집 결과: 10개 장소
- 수집 정보: 순위, 업체명, 원시 텍스트
- 파일 형식: CSV (UTF-8 BOM)

================================================================================
🔚 결론
================================================================================

이 네이버 플레이스 순위 크롤링 시스템은 다음과 같은 특징을 가집니다:

✅ 강점:
- 최신 Playwright 1.52.0 기반 안정적 크롤링
- 다중 fallback 전략으로 높은 성공률
- 환경별 자동 최적화 (개발/프로덕션)
- 포괄적인 스텔스 기법 적용
- 다양한 출력 형식 지원 (CSV, JSON, Excel)

⚠️ 주의사항:
- 네이버 이용약관 준수 필수
- 정기적인 선택자 업데이트 필요
- 적절한 요청 간격 유지 (3초 권장)

🚀 사용 권장사항:
- 개발 시: headless=False로 시각적 확인
- 프로덕션: 환경변수 기반 자동 최적화
- 배치 작업: 키워드 간 충분한 대기 시간
- 모니터링: 에러율 및 성공률 지속 관찰

================================================================================
💬 문의 및 지원
================================================================================
기술적 문제 또는 개선사항이 있을 경우:
1. debug_html/ 폴더의 HTML 구조 분석 결과 확인
2. 선택자 배열에 새로운 패턴 추가
3. 스텔스 설정 조정
4. 타임아웃 값 조정

================================================================================
🏁 END OF DOCUMENT
================================================================================